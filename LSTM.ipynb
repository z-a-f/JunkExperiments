{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import quantized as nnq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"Single LSTM cell\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_size = input_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        self.igates = nn.Linear(input_dim, 4 * hidden_dim, bias=True)\n",
    "        self.hgates = nn.Linear(hidden_dim, 4 * hidden_dim, bias=True)  # Maybe we don't need bias here\n",
    "        self.gates = nnq.FloatFunctional()\n",
    "        \n",
    "        self.fgate_cx = nnq.FloatFunctional()\n",
    "        self.igate_cgate = nnq.FloatFunctional()\n",
    "        self.fgate_cx_igate_cgate = nnq.FloatFunctional()\n",
    "        \n",
    "        self.ogate_cy = nnq.FloatFunctional()\n",
    "        \n",
    "    def forward(self, x, hidden=None):  # (Batch, inputSize), ((Batch, hiddenSize), (Batch, hiddenSize))\n",
    "        if hidden is None:\n",
    "            hidden = self.initialize_hidden(x.shape[0], x.is_quantized)\n",
    "        hx, cx = hidden\n",
    "\n",
    "        igates = self.igates(x)  # (Batch, 4*hiddenSize)\n",
    "        hgates = self.hgates(hx) # (Batch, 4*hiddenSize)\n",
    "        gates = self.gates.add(igates, hgates)  # (Batch, 4*hiddenSize)\n",
    "        \n",
    "        input_gate, forget_gate, cell_gate, out_gate = gates.chunk(4, 1) # (Batch, hiddenSize) x 4\n",
    "        \n",
    "        input_gate = torch.sigmoid(input_gate)\n",
    "        forget_gate = torch.sigmoid(forget_gate)\n",
    "        cell_gate = torch.tanh(cell_gate)\n",
    "        out_gate = torch.sigmoid(out_gate)\n",
    "        \n",
    "        fgate_cx = self.fgate_cx.mul(forget_gate, cx)  # (Batch, hiddenSize)\n",
    "        igate_cgate = self.igate_cgate.mul(input_gate, cell_gate)  # (Batch, hiddenSize)\n",
    "        fgate_cx_igate_cgate = self.fgate_cx_igate_cgate.add(fgate_cx, igate_cgate)  # (Batch, hiddenSize)\n",
    "        cy = fgate_cx_igate_cgate\n",
    "        \n",
    "        tanh_cy = F.tanh(cy)\n",
    "        hy = self.ogate_cy.mul(out_gate, tanh_cy)  # (Batch, hiddenSize)\n",
    "        \n",
    "        return hy, cy   # (Batch, hiddenSize), (Batch, hiddenSize)\n",
    "    \n",
    "    def initialize_hidden(self, batch_size, is_quantized):\n",
    "        h, c = torch.zeros((batch_size, self.hidden_size)), torch.zeros((batch_size, self.hidden_size))\n",
    "        if is_quantized:\n",
    "            h = torch.quantize_per_tensor(h, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "            c = torch.quantize_per_tensor(c, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "S = 128\n",
    "iS = 7\n",
    "\n",
    "x = torch.randn(B, iS)\n",
    "\n",
    "lstm = LSTMCell(iS, 32)\n",
    "hy, cy = lstm(x)\n",
    "print(hy.shape, cy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStack(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers):\n",
    "        \"\"\"\n",
    "        stack of lstms has num_layers cells\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        if not isinstance(hidden_sizes, (list, tuple)):\n",
    "            hidden_sizes = [hidden_sizes] * self.num_layers\n",
    "        assert(len(hidden_sizes) == num_layers)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "            \n",
    "        cells = [LSTMCell(input_size, hidden_sizes[0])]\n",
    "        \n",
    "        for idx in range(1, self.num_layers):\n",
    "            cells.append(LSTMCell(hidden_sizes[idx-1], hidden_sizes[idx]))\n",
    "        self.cells = nn.ModuleList(cells)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.initialize_hidden(x.shape[0], x.is_quantized)\n",
    "        hx, cx = hidden\n",
    "\n",
    "        for idx, cell in enumerate(self.cells):\n",
    "            x, cy = cell(x, (hx[idx], cx[idx]))\n",
    "            hx[idx] = x\n",
    "            cx[idx] = cy\n",
    "        return hx, cx\n",
    "            \n",
    "    def initialize_hidden(self, batch_size, quantized):\n",
    "        hc = [ cell.initialize_hidden(batch_size, quantized) for cell in self.cells ]\n",
    "        hx, cx = zip(*hc)\n",
    "        return list(hx), list(cx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "S = 128\n",
    "iS = 7\n",
    "num_layers = 3\n",
    "\n",
    "x = torch.randn(B, iS)\n",
    "\n",
    "lstm = LSTMStack(iS, 32, num_layers)\n",
    "hy, cy = lstm(x)\n",
    "print(len(hy), len(cy))\n",
    "print(hy[0].shape, cy[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers):\n",
    "        super().__init__()\n",
    "        self.cell_stack = LSTMStack(input_size, hidden_sizes, num_layers)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.cell_stack.initialize_hidden(x.shape[1], x.is_quantized)\n",
    "        seq_len = x.shape[0]\n",
    "        y = []\n",
    "        for idx in range(seq_len):\n",
    "            hidden = self.cell_stack(x[idx], hidden)\n",
    "            y.append(hidden[0][-1])\n",
    "        return torch.stack(y, 0), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 7]) torch.Size([128, 16, 32])\n",
      "3 3\n",
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "S = 128\n",
    "iS = 7\n",
    "num_layers = 3\n",
    "\n",
    "x = torch.randn(S, B, iS)\n",
    "\n",
    "lstm = LSTM(iS, 32, num_layers)\n",
    "y, (hy, cy) = lstm(x)\n",
    "print(x.shape, y.shape)\n",
    "print(len(hy), len(cy))\n",
    "print(hy[0].shape, cy[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "ref_lstm = nn.LSTM(iS, 32, num_layers)\n",
    "y_ref = ref_lstm(x)\n",
    "print(y_ref[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization as tq\n",
    "\n",
    "batch_size = 7\n",
    "seq_len = 257\n",
    "input_size = 31\n",
    "hidden_size = 61\n",
    "num_layers = 5\n",
    "\n",
    "x = torch.randn(seq_len, batch_size, input_size)\n",
    "qx = torch.quantize_per_tensor(x, scale=1e-2, zero_point=128, dtype=torch.quint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch-maxpool/torch/quantization/observer.py:120: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(input_size, hidden_size, num_layers)\n",
    "y, (hy, cy) = lstm(x)\n",
    "\n",
    "# 1. Prepare\n",
    "lstm.qconfig = torch.quantization.default_qconfig\n",
    "lstm_prepared = tq.prepare(lstm, inplace=False)\n",
    "\n",
    "# 2. Calibrate\n",
    "lstm_prepared(x);\n",
    "\n",
    "# 3. Convert\n",
    "lstm_converted = tq.convert(lstm_prepared, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([257, 7, 61])\n"
     ]
    }
   ],
   "source": [
    "qy, hidden = lstm_converted(qx)\n",
    "print(qy.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
