{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import quantized as nnq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"Single LSTM cell\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_size = input_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        self.igates = nn.Linear(input_dim, 4 * hidden_dim, bias=True)\n",
    "        self.hgates = nn.Linear(hidden_dim, 4 * hidden_dim, bias=True)  # Maybe we don't need bias here\n",
    "        self.gates = nnq.FloatFunctional()\n",
    "        \n",
    "        self.fgate_cx = nnq.FloatFunctional()\n",
    "        self.igate_cgate = nnq.FloatFunctional()\n",
    "        self.fgate_cx_igate_cgate = nnq.FloatFunctional()\n",
    "        \n",
    "        self.ogate_cy = nnq.FloatFunctional()\n",
    "        \n",
    "    def forward(self, x, hidden=None):  # (Batch, inputSize), ((Batch, hiddenSize), (Batch, hiddenSize))\n",
    "        if hidden is None:\n",
    "            hidden = self.initialize_hidden(x.shape[0], x.is_quantized)\n",
    "        hx, cx = hidden\n",
    "\n",
    "        igates = self.igates(x)  # (Batch, 4*hiddenSize)\n",
    "        hgates = self.hgates(hx) # (Batch, 4*hiddenSize)\n",
    "        gates = self.gates.add(igates, hgates)  # (Batch, 4*hiddenSize)\n",
    "        \n",
    "        input_gate, forget_gate, cell_gate, out_gate = gates.chunk(4, 1) # (Batch, hiddenSize) x 4\n",
    "        \n",
    "        input_gate = torch.sigmoid(input_gate)\n",
    "        forget_gate = torch.sigmoid(forget_gate)\n",
    "        cell_gate = torch.tanh(cell_gate)\n",
    "        out_gate = torch.sigmoid(out_gate)\n",
    "        \n",
    "        fgate_cx = self.fgate_cx.mul(forget_gate, cx)  # (Batch, hiddenSize)\n",
    "        igate_cgate = self.igate_cgate.mul(input_gate, cell_gate)  # (Batch, hiddenSize)\n",
    "        fgate_cx_igate_cgate = self.fgate_cx_igate_cgate.add(fgate_cx, igate_cgate)  # (Batch, hiddenSize)\n",
    "        cy = fgate_cx_igate_cgate\n",
    "        \n",
    "        tanh_cy = F.tanh(cy)\n",
    "        hy = self.ogate_cy.mul(out_gate, tanh_cy)  # (Batch, hiddenSize)\n",
    "        \n",
    "        return hy, cy   # (Batch, hiddenSize), (Batch, hiddenSize)\n",
    "    \n",
    "    def initialize_hidden(self, batch_size, is_quantized):\n",
    "        h, c = torch.zeros((batch_size, self.hidden_size)), torch.zeros((batch_size, self.hidden_size))\n",
    "        if is_quantized:\n",
    "            h = torch.quantize_per_tensor(h, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "            c = torch.quantize_per_tensor(c, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/functional.py:1629: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "S = 128\n",
    "iS = 7\n",
    "\n",
    "x = torch.randn(B, iS)\n",
    "\n",
    "lstm = LSTMCell(iS, 32)\n",
    "hy, cy = lstm(x)\n",
    "print(hy.shape, cy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStack(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers):\n",
    "        \"\"\"\n",
    "        stack of lstms has num_layers cells\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        if not isinstance(hidden_sizes, (list, tuple)):\n",
    "            hidden_sizes = [hidden_sizes] * self.num_layers\n",
    "        assert(len(hidden_sizes) == num_layers)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "            \n",
    "        cells = [LSTMCell(input_size, hidden_sizes[0])]\n",
    "        \n",
    "        for idx in range(1, self.num_layers):\n",
    "            cells.append(LSTMCell(hidden_sizes[idx-1], hidden_sizes[idx]))\n",
    "        self.cells = nn.ModuleList(cells)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.initialize_hidden(x.shape[0], x.is_quantized)\n",
    "        hx, cx = hidden\n",
    "\n",
    "        for idx, cell in enumerate(self.cells):\n",
    "            x, cy = cell(x, (hx[idx], cx[idx]))\n",
    "            hx[idx] = x\n",
    "            cx[idx] = cy\n",
    "        return hx, cx\n",
    "            \n",
    "    def initialize_hidden(self, batch_size, quantized):\n",
    "        hc = [ cell.initialize_hidden(batch_size, quantized) for cell in self.cells ]\n",
    "        hx, cx = zip(*hc)\n",
    "        return list(hx), list(cx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "S = 128\n",
    "iS = 7\n",
    "num_layers = 3\n",
    "\n",
    "x = torch.randn(B, iS)\n",
    "\n",
    "lstm = LSTMStack(iS, 32, num_layers)\n",
    "hy, cy = lstm(x)\n",
    "print(len(hy), len(cy))\n",
    "print(hy[0].shape, cy[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers):\n",
    "        super().__init__()\n",
    "        self.cell_stack = LSTMStack(input_size, hidden_sizes, num_layers)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.cell_stack.initialize_hidden(x.shape[1], x.is_quantized)\n",
    "        seq_len = x.shape[0]\n",
    "        y = []\n",
    "        for idx in range(seq_len):\n",
    "            hidden = self.cell_stack(x[idx], hidden)\n",
    "            y.append(hidden[0][-1])\n",
    "        return torch.stack(y, 0), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 7]) torch.Size([128, 16, 32])\n",
      "3 3\n",
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "S = 128\n",
    "iS = 7\n",
    "num_layers = 3\n",
    "\n",
    "x = torch.randn(S, B, iS)\n",
    "\n",
    "lstm = LSTM(iS, 32, num_layers)\n",
    "y, (hy, cy) = lstm(x)\n",
    "print(x.shape, y.shape)\n",
    "print(len(hy), len(cy))\n",
    "print(hy[0].shape, cy[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "ref_lstm = nn.LSTM(iS, 32, num_layers)\n",
    "y_ref = ref_lstm(x)\n",
    "print(y_ref[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization as tq\n",
    "\n",
    "batch_size = 7\n",
    "seq_len = 257\n",
    "input_size = 31\n",
    "hidden_size = 61\n",
    "num_layers = 5\n",
    "\n",
    "x = torch.randn(seq_len, batch_size, input_size)\n",
    "qx = torch.quantize_per_tensor(x, scale=1e-2, zero_point=128, dtype=torch.quint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch-lstm/torch/quantization/observer.py:120: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(input_size, hidden_size, num_layers)\n",
    "y, (hy, cy) = lstm(x)\n",
    "lstm.eval()\n",
    "\n",
    "# 1. Prepare\n",
    "lstm.qconfig = torch.quantization.default_qconfig\n",
    "lstm_prepared = tq.prepare(lstm, inplace=False)\n",
    "\n",
    "# 2. Calibrate\n",
    "with torch.no_grad():\n",
    "    lstm_prepared(x)\n",
    "\n",
    "# 3. Convert\n",
    "lstm_converted = tq.convert(lstm_prepared, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([257, 7, 61])\n"
     ]
    }
   ],
   "source": [
    "qy, hidden = lstm_converted(qx)\n",
    "print(qy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(32, 32, kernel_size=1, padding=0)\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=32, batch_first=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(1, -1, 32)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.lstm(x)[0]\n",
    "        return x\n",
    "    \n",
    "def calibrate(model, calib_data):\n",
    "    with torch.no_grad():\n",
    "        for x in calib_data:\n",
    "            model(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self : __torch__.___torch_mangle_49.Model,\n",
       "      %x.2 : Tensor):\n",
       "  %2 : float = prim::Constant[value=0.]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:582:30\n",
       "  %3 : bool = prim::Constant[value=1]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:581:61\n",
       "  %4 : int = prim::Constant[value=32]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:571:48\n",
       "  %batch_sizes.1 : None = prim::Constant() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:563:26\n",
       "  %6 : str = prim::Constant[value=\"input.size(-1) must be equal to input_size. Expected {}, got {}\"]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:179:16\n",
       "  %7 : int = prim::Constant[value=-1]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:177:41\n",
       "  %8 : str = prim::Constant[value=\"input must have {} dimensions, got {}\"]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:175:16\n",
       "  %9 : int = prim::Constant[value=3]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:172:63\n",
       "  %10 : str = prim::Constant[value=\"Expected hidden[0] size {}, got {}\"]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:534:31\n",
       "  %11 : str = prim::Constant[value=\"Expected hidden[1] size {}, got {}\"]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:536:31\n",
       "  %12 : int[] = prim::Constant[value=[1, 0, 2]]()\n",
       "  %13 : int[] = prim::Constant[value=[1, -1, 32]]()\n",
       "  %14 : QInt8(32:32, 32:1, 1:1, 1:1, requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>]()\n",
       "  %15 : int[] = prim::Constant[value=[0, 0]]()\n",
       "  %16 : int[] = prim::Constant[value=[1, 1]]()\n",
       "  %17 : int = prim::Constant[value=1]() # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/conv.py:419:50\n",
       "  %self.lstm.training : bool = prim::Constant[value=0]()\n",
       "  %self.lstm._flat_weights : Tensor[] = prim::Constant[value=[<Tensors>]]()\n",
       "  %self.x.4_zero_point_0 : int = prim::Constant[value=59]()\n",
       "  %self.x.4_scale_0 : float = prim::Constant[value=0.015116374008357525]()\n",
       "  %self.conv.bias : Float(32:1, requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>]()\n",
       "  %self.x.2_scalar_type_0 : int = prim::Constant[value=13]()\n",
       "  %self.x.2_zero_point_0 : int = prim::Constant[value=0]()\n",
       "  %self.x.2_scale_0 : float = prim::Constant[value=0.0078664161264896393]()\n",
       "  %x.2.quant : Tensor = aten::quantize_per_tensor(%x.2, %self.x.2_scale_0, %self.x.2_zero_point_0, %self.x.2_scalar_type_0)\n",
       "  %27 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized._jit_pass_packed_weight_0\"](%self)\n",
       "  %28 : Tensor = quantized::conv2d(%x.2.quant, %27, %self.x.4_scale_0, %self.x.4_zero_point_0)\n",
       "  %x.5 : Tensor = aten::reshape(%28, %13) # <ipython-input-13-441655026a28>:9:12\n",
       "  %x.7 : Tensor = aten::permute(%x.5, %12) # <ipython-input-13-441655026a28>:10:12\n",
       "  %31 : int = aten::size(%x.7, %17) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:564:68\n",
       "  %x.7.dequant.1 : Tensor = aten::dequantize(%x.7)\n",
       "  %33 : int = prim::dtype(%x.7.dequant.1)\n",
       "  %34 : Device = prim::device(%x.7.dequant.1)\n",
       "  %35 : int[] = prim::ListConstruct(%17, %31, %4)\n",
       "  %zeros.1 : Tensor = aten::zeros(%35, %33, %batch_sizes.1, %34, %batch_sizes.1) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:570:20\n",
       "  %hx.3 : (Tensor, Tensor) = prim::TupleConstruct(%zeros.1, %zeros.1)\n",
       "  %38 : int = aten::dim(%x.7) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:173:11\n",
       "  %39 : bool = aten::ne(%38, %9) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:173:11\n",
       "   = prim::If(%39) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:173:8\n",
       "    block0():\n",
       "      %40 : str = aten::format(%8, %9, %38) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:175:16\n",
       "       = prim::RaiseException(%40) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:174:12\n",
       "      -> ()\n",
       "    block1():\n",
       "      -> ()\n",
       "  %41 : int = aten::size(%x.7, %7) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:177:30\n",
       "  %42 : bool = aten::ne(%4, %41) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:177:11\n",
       "   = prim::If(%42) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:177:8\n",
       "    block0():\n",
       "      %43 : str = aten::format(%6, %4, %41) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:179:16\n",
       "       = prim::RaiseException(%43) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:178:12\n",
       "      -> ()\n",
       "    block1():\n",
       "      -> ()\n",
       "  %expected_hidden_size.2 : (int, int, int) = prim::TupleConstruct(%17, %31, %4)\n",
       "  %45 : Tensor = prim::TupleIndex(%hx.3, %self.x.2_zero_point_0)\n",
       "  %46 : int[] = aten::size(%45) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:195:11\n",
       "  %47 : int, %48 : int, %49 : int = prim::TupleUnpack(%expected_hidden_size.2)\n",
       "  %50 : int[] = prim::ListConstruct(%47, %48, %49)\n",
       "  %51 : bool = aten::ne(%46, %50) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:195:11\n",
       "   = prim::If(%51) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:195:8\n",
       "    block0():\n",
       "      %52 : int[] = aten::list(%46) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:196:64\n",
       "      %53 : str = aten::format(%10, %expected_hidden_size.2, %52) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:196:31\n",
       "       = prim::RaiseException(%53) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:196:12\n",
       "      -> ()\n",
       "    block1():\n",
       "      -> ()\n",
       "  %54 : Tensor = prim::TupleIndex(%hx.3, %17)\n",
       "  %55 : int[] = aten::size(%54) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:195:11\n",
       "  %56 : bool = aten::ne(%55, %50) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:195:11\n",
       "   = prim::If(%56) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:195:8\n",
       "    block0():\n",
       "      %57 : int[] = aten::list(%55) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:196:64\n",
       "      %58 : str = aten::format(%11, %expected_hidden_size.2, %57) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:196:31\n",
       "       = prim::RaiseException(%58) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:196:12\n",
       "      -> ()\n",
       "    block1():\n",
       "      -> ()\n",
       "  %59 : Tensor, %60 : Tensor = prim::TupleUnpack(%hx.3)\n",
       "  %61 : Tensor[] = prim::ListConstruct(%59, %60)\n",
       "  %62 : Tensor, %63 : Tensor, %64 : Tensor = aten::lstm(%x.7.dequant.1, %61, %self.lstm._flat_weights, %3, %17, %2, %self.lstm.training, %self.lstm.training, %self.lstm.training) # /home/zafar/Git/pytorch-dev/pytorch-lstm/torch/nn/modules/rnn.py:581:21\n",
       "  %65 : (Tensor, Tensor) = prim::TupleConstruct(%63, %64)\n",
       "  %66 : (Tensor, (Tensor, Tensor)) = prim::TupleConstruct(%62, %65)\n",
       "  %x.9 : Tensor = prim::TupleIndex(%66, %self.x.2_zero_point_0)\n",
       "  return (%x.9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import jit\n",
    "\n",
    "model = jit.script(Model())\n",
    "model.eval()\n",
    "qm = tq.quantize_jit(model, {'': tq.default_qconfig}, calibrate, [[torch.rand(1, 32, 1, 10)]])\n",
    "\n",
    "qm.inlined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModuleImpl(\n",
       "  (conv): QuantizedConv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.027948811650276184, zero_point=63)\n",
       "  (lstm): LSTM(32, 32)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import fx\n",
    "model = fx.symbolic_trace(Model())\n",
    "model.eval()\n",
    "\n",
    "# 1. Prepare\n",
    "model_prepared = tq.prepare_fx(model, {'': tq.default_qconfig})\n",
    "\n",
    "# 2. Calibrate\n",
    "x = torch.randn(1, 32, 1, 10)\n",
    "for _ in range(5):\n",
    "    model_prepared(x);\n",
    "model_converted = tq.convert_fx(model_prepared)\n",
    "model_converted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
