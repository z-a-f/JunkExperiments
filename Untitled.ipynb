{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qparams(xf, qtype):\n",
    "    fmin = min(xf.min(), 0)\n",
    "    fmax = max(xf.max(), 0)\n",
    "    if fmin == fmax == 0:\n",
    "        return 1.0, 0\n",
    "    info = torch.iinfo(qtype)\n",
    "    qmin = info.min\n",
    "    qmax = info.max\n",
    "    \n",
    "    scale = (fmax - fmin) / (qmax - qmin)\n",
    "    zero_point = qmax - fmax / scale\n",
    "    return scale, int(zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fused Linear\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "try:\n",
    "    from typing_extensions import Final\n",
    "except:\n",
    "    from torch.jit import Final\n",
    "\n",
    "iC = 3\n",
    "oC = 5\n",
    "\n",
    "linear_models = []\n",
    "\n",
    "# nn.Linear + nn.ReLU\n",
    "class LinearReLU1(nn.Module):\n",
    "    method_scheme: Final = 'nn.Linear + nn.ReLU'\n",
    "    def __init__(self):\n",
    "        super(LinearReLU1, self).__init__()\n",
    "        self.linear = nn.Linear(iC, oC)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "linear_models.append(LinearReLU1())\n",
    "\n",
    "# nn.Linear + F.relu\n",
    "class LinearReLU2(nn.Module):\n",
    "    method_scheme: Final = 'nn.Linear + F.relu'\n",
    "    def __init__(self):\n",
    "        super(LinearReLU2, self).__init__()\n",
    "        self.linear = nn.Linear(iC, oC)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.linear(x))\n",
    "linear_models.append(LinearReLU2())\n",
    "\n",
    "# F.linear + nn.ReLU\n",
    "class LinearReLU3(nn.Module):\n",
    "    method_scheme: Final = 'F.linear + nn.ReLU'\n",
    "    def __init__(self, w, b):\n",
    "        super(LinearReLU3, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    def forward(self, x):\n",
    "        return self.relu(F.linear(x, self.w, self.b))\n",
    "linear_models.append(LinearReLU3(torch.randn(oC, iC), torch.randn(oC)))\n",
    "\n",
    "# F.linear + F.relu\n",
    "class LinearReLU4(nn.Module):\n",
    "    method_scheme: Final = 'F.linear + F.relu'\n",
    "    def __init__(self, w, b):\n",
    "        super(LinearReLU4, self).__init__()\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    def forward(self, x):\n",
    "        return F.relu(F.linear(x, self.w, self.b))\n",
    "linear_models.append(LinearReLU4(torch.randn(oC, iC), torch.randn(oC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scripted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f05296cfb94a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscripted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scripted' is not defined"
     ]
    }
   ],
   "source": [
    "scripted.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = scripted.graph_for(torch.randn(iC))\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 0 === nn.Linear + nn.ReLU === \n",
      "\n",
      "module __torch__.LinearReLU1 {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    linear = <__torch__.torch.nn.modules.linear.Linear object at 0x55f06e4efa60>\n",
      "    relu = <__torch__.torch.nn.modules.activation.ReLU object at 0x55f06e525330>\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.LinearReLU1,\n",
      "            %x.1 : Tensor):\n",
      "        %2 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name=\"relu\"](%self)\n",
      "        %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self)\n",
      "        %5 : Tensor = prim::CallMethod[name=\"forward\"](%3, %x.1) # <ipython-input-4-f3a59a5677f9>:24:25\n",
      "        %6 : Tensor = prim::CallMethod[name=\"forward\"](%2, %5) # <ipython-input-4-f3a59a5677f9>:24:15\n",
      "        return (%6)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "    module __torch__.torch.nn.modules.linear.Linear {\n",
      "      parameters {\n",
      "        weight = ...\n",
      "        bias = ...\n",
      "      }\n",
      "      attributes {\n",
      "        weight = ...\n",
      "        bias = ...\n",
      "        training = True\n",
      "      }\n",
      "      methods {\n",
      "        method forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.linear.Linear,\n",
      "                %input.1 : Tensor):\n",
      "            %5 : Function = prim::Constant[name=\"linear\"]()\n",
      "            %3 : Tensor = prim::GetAttr[name=\"weight\"](%self)\n",
      "            %4 : Tensor = prim::GetAttr[name=\"bias\"](%self)\n",
      "            %6 : Tensor = prim::CallFunction(%5, %input.1, %3, %4) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/linear.py:87:15\n",
      "            return (%6)\n",
      "      \n",
      "        }\n",
      "      }\n",
      "      submodules {\n",
      "      }\n",
      "    }\n",
      "    module __torch__.torch.nn.modules.activation.ReLU {\n",
      "      parameters {\n",
      "      }\n",
      "      attributes {\n",
      "        training = True\n",
      "      }\n",
      "      methods {\n",
      "        method forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.activation.ReLU,\n",
      "                %input.1 : Tensor):\n",
      "            %4 : Function = prim::Constant[name=\"relu\"]()\n",
      "            %3 : bool = prim::Constant[value=0]() # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:97:37\n",
      "            %5 : Tensor = prim::CallFunction(%4, %input.1, %3) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:97:15\n",
      "            return (%5)\n",
      "      \n",
      "        }\n",
      "      }\n",
      "      submodules {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "=== 1 === nn.Linear + F.relu === \n",
      "\n",
      "module __torch__.LinearReLU2 {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    linear = <__torch__.torch.nn.modules.linear.Linear object at 0x55f06e529230>\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.LinearReLU2,\n",
      "            %x.1 : Tensor):\n",
      "        %6 : Function = prim::Constant[name=\"relu\"]()\n",
      "        %5 : bool = prim::Constant[value=0]()\n",
      "        %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self)\n",
      "        %4 : Tensor = prim::CallMethod[name=\"forward\"](%2, %x.1) # <ipython-input-4-f3a59a5677f9>:34:22\n",
      "        %7 : Tensor = prim::CallFunction(%6, %4, %5) # <ipython-input-4-f3a59a5677f9>:34:15\n",
      "        return (%7)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "    module __torch__.torch.nn.modules.linear.Linear {\n",
      "      parameters {\n",
      "        weight = ...\n",
      "        bias = ...\n",
      "      }\n",
      "      attributes {\n",
      "        weight = ...\n",
      "        bias = ...\n",
      "        training = True\n",
      "      }\n",
      "      methods {\n",
      "        method forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.linear.Linear,\n",
      "                %input.1 : Tensor):\n",
      "            %5 : Function = prim::Constant[name=\"linear\"]()\n",
      "            %3 : Tensor = prim::GetAttr[name=\"weight\"](%self)\n",
      "            %4 : Tensor = prim::GetAttr[name=\"bias\"](%self)\n",
      "            %6 : Tensor = prim::CallFunction(%5, %input.1, %3, %4) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/linear.py:87:15\n",
      "            return (%6)\n",
      "      \n",
      "        }\n",
      "      }\n",
      "      submodules {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "=== 2 === F.linear + nn.ReLU === \n",
      "\n",
      "module __torch__.LinearReLU3 {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    w = ...\n",
      "    b = ...\n",
      "    relu = <__torch__.torch.nn.modules.activation.ReLU object at 0x55f06e528a80>\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.LinearReLU3,\n",
      "            %x.1 : Tensor):\n",
      "        %6 : Function = prim::Constant[name=\"linear\"]()\n",
      "        %2 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name=\"relu\"](%self)\n",
      "        %4 : Tensor = prim::GetAttr[name=\"w\"](%self)\n",
      "        %5 : Tensor = prim::GetAttr[name=\"b\"](%self)\n",
      "        %7 : Tensor = prim::CallFunction(%6, %x.1, %4, %5) # <ipython-input-4-f3a59a5677f9>:46:25\n",
      "        %8 : Tensor = prim::CallMethod[name=\"forward\"](%2, %7) # <ipython-input-4-f3a59a5677f9>:46:15\n",
      "        return (%8)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "    module __torch__.torch.nn.modules.activation.ReLU {\n",
      "      parameters {\n",
      "      }\n",
      "      attributes {\n",
      "        training = True\n",
      "      }\n",
      "      methods {\n",
      "        method forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.activation.ReLU,\n",
      "                %input.1 : Tensor):\n",
      "            %4 : Function = prim::Constant[name=\"relu\"]()\n",
      "            %3 : bool = prim::Constant[value=0]() # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:97:37\n",
      "            %5 : Tensor = prim::CallFunction(%4, %input.1, %3) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:97:15\n",
      "            return (%5)\n",
      "      \n",
      "        }\n",
      "      }\n",
      "      submodules {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "=== 3 === F.linear + F.relu === \n",
      "\n",
      "module __torch__.LinearReLU4 {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    w = ...\n",
      "    b = ...\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.LinearReLU4,\n",
      "            %x.1 : Tensor):\n",
      "        %8 : Function = prim::Constant[name=\"relu\"]()\n",
      "        %7 : bool = prim::Constant[value=0]()\n",
      "        %5 : Function = prim::Constant[name=\"linear\"]()\n",
      "        %3 : Tensor = prim::GetAttr[name=\"w\"](%self)\n",
      "        %4 : Tensor = prim::GetAttr[name=\"b\"](%self)\n",
      "        %6 : Tensor = prim::CallFunction(%5, %x.1, %3, %4) # <ipython-input-4-f3a59a5677f9>:57:22\n",
      "        %9 : Tensor = prim::CallFunction(%8, %6, %7) # <ipython-input-4-f3a59a5677f9>:57:15\n",
      "        return (%9)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, model in enumerate(linear_models):\n",
    "    print(f'\\n=== {idx} === {model.method_scheme} === \\n')\n",
    "    scripted = torch.jit.script(model)\n",
    "    print(scripted._c.dump_to_str(True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[[-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.],\n",
      "         [ -3.,  -2.,  -1.,   0.,   1.,   2.,   3.],\n",
      "         [  4.,   5.,   6.,   7.,   8.,   9.,  10.]]])\n",
      "qx = tensor([[[-10.0392,  -9.0196,  -8.0000,  -6.9804,  -5.9608,  -5.0196,  -4.0000],\n",
      "         [ -2.9804,  -2.0392,  -1.0196,   0.0000,   1.0196,   2.0392,   2.9804],\n",
      "         [  4.0000,   5.0196,   5.9608,   6.9804,   8.0000,   9.0196,   9.9608]]],\n",
      "       size=(1, 3, 7), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0784313753247261,\n",
      "       zero_point=0)\n",
      "module __torch__.torch.nn.modules.padding.ReflectionPad1d {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.torch.nn.modules.padding.ReflectionPad1d,\n",
      "            %input.1 : Tensor):\n",
      "        %9 : Function = prim::Constant[name=\"_pad\"]()\n",
      "        %8 : float = prim::Constant[value=0.]()\n",
      "        %6 : str = prim::Constant[value=\"reflect\"]() # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/padding.py:163:42\n",
      "        %3 : int = prim::Constant[value=3]() # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/padding.py:163:28\n",
      "        %7 : int[] = prim::ListConstruct(%3, %3)\n",
      "        %10 : Tensor = prim::CallFunction(%9, %input.1, %7, %6, %8) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/padding.py:163:15\n",
      "        return (%10)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/tensor.py:357: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "# Reflection pad\n",
    "from torch import nn\n",
    "\n",
    "N = 1\n",
    "C = 3\n",
    "Win = 7\n",
    "\n",
    "numel = N * C * Win\n",
    "\n",
    "x = torch.arange(numel).to(torch.float) - numel // 2\n",
    "x = x.resize(N, C, Win)\n",
    "print(f'x = {x}')\n",
    "\n",
    "s, zp = qparams(x, torch.qint8)\n",
    "qx = torch.quantize_per_tensor(x, s, zp, torch.qint8)\n",
    "print(f'qx = {qx}')\n",
    "\n",
    "reflection_pad1d = nn.ReflectionPad1d(3)\n",
    "# class ReflectionPad1d(nn.Module):\n",
    "#     def __init__(self, padding):\n",
    "#         super(ReflectionPad1d, self).__init__()\n",
    "#         self.padding = padding\n",
    "#     def forward(self, x):\n",
    "#         x = F.pad(x, (self.padding, self.padding), 'reflect')\n",
    "# reflection_pad1d = ReflectionPad1d(3)\n",
    "\n",
    "reflection_pad1d(qx)\n",
    "scripted = torch.jit.script(reflection_pad1d)\n",
    "print(scripted._c.dump_to_str(True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[-12., -11., -10.,  -9.],\n",
      "        [ -8.,  -7.,  -6.,  -5.],\n",
      "        [ -4.,  -3.,  -2.,  -1.],\n",
      "        [  0.,   1.,   2.,   3.],\n",
      "        [  4.,   5.,   6.,   7.],\n",
      "        [  8.,   9.,  10.,  11.]])\n",
      "tanh(x) = tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -0.9999],\n",
      "        [-0.9993, -0.9951, -0.9640, -0.7616],\n",
      "        [ 0.0000,  0.7616,  0.9640,  0.9951],\n",
      "        [ 0.9993,  0.9999,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000]])\n",
      "qx = tensor([[-11.9961, -11.0039, -10.0118,  -9.0196],\n",
      "        [ -8.0275,  -7.0353,  -6.0431,  -4.9608],\n",
      "        [ -3.9686,  -2.9765,  -1.9843,  -0.9922],\n",
      "        [  0.0000,   0.9922,   1.9843,   2.9765],\n",
      "        [  3.9686,   4.9608,   6.0431,   7.0353],\n",
      "        [  8.0275,   9.0196,  10.0118,  11.0039]], size=(6, 4),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.09019608050584793, zero_point=5)\n",
      "tanh(qx) = tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -0.9922, -0.9609, -0.7578],\n",
      "        [ 0.0000,  0.7578,  0.9609,  0.9922],\n",
      "        [ 0.9922,  0.9922,  0.9922,  0.9922],\n",
      "        [ 0.9922,  0.9922,  0.9922,  0.9922]], size=(6, 4), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0078125,\n",
      "       zero_point=0)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(-12, 12).to(torch.float)\n",
    "x = x.resize(6, 4)\n",
    "print(f'x = {x}\\ntanh(x) = {torch.tanh(x)}')\n",
    "\n",
    "s, zp = qparams(x, torch.qint8)\n",
    "qx = torch.quantize_per_tensor(x, s, zp, torch.qint8)\n",
    "print(f'qx = {qx}\\ntanh(qx) = {torch.tanh(qx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-12., -11., -10.,  -9.],\n",
       "         [ -8.,  -7.,  -6.,  -5.],\n",
       "         [ -4.,  -3.,  -2.,  -1.],\n",
       "         [  0.,   1.,   2.,   3.],\n",
       "         [  4.,   5.,   6.,   7.],\n",
       "         [  8.,   9.,  10.,  11.]]),\n",
       " tensor([[-11.9961, -11.0039, -10.0118,  -9.0196,  -8.0275,  -7.0353,  -6.0431,\n",
       "           -4.9608],\n",
       "         [ -3.9686,  -2.9765,  -1.9843,  -0.9922,   0.0000,   0.9922,   1.9843,\n",
       "            2.9765],\n",
       "         [  3.9686,   4.9608,   6.0431,   7.0353,   8.0275,   9.0196,  10.0118,\n",
       "           11.0039]], size=(3, 8), dtype=torch.qint8,\n",
       "        quantization_scheme=torch.per_tensor_affine, scale=0.09019608050584793,\n",
       "        zero_point=5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, qx.resize_(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module __torch__.Resize {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    new_size = (3, 8)\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.Resize,\n",
      "            %x.1 : Tensor):\n",
      "        %7 : None = prim::Constant()\n",
      "        %3 : int = prim::Constant[value=1]() # <ipython-input-10-7ee9b0023ab2>:10:22\n",
      "        %5 : int = aten::numel(%x.1) # <ipython-input-10-7ee9b0023ab2>:10:28\n",
      "        %6 : int[] = prim::ListConstruct(%3, %3, %5)\n",
      "        %x.4 : Tensor = aten::resize_(%x.1, %6, %7) # <ipython-input-10-7ee9b0023ab2>:10:12\n",
      "        return (%x.4)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Resize(nn.Module):\n",
    "    def __init__(self, new_size):\n",
    "        super(Resize, self).__init__()\n",
    "        self.new_size = new_size\n",
    "    def forward(self, x):\n",
    "        x = x.resize_(1, 1, x.numel())\n",
    "        return x\n",
    "    \n",
    "resize = Resize((3, 8))\n",
    "scripted = torch.jit.script(resize)\n",
    "print(scripted._c.dump_to_str(True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UnsupportedNodeError",
     "evalue": "Lambda aren't supported:\n  File \"/home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/autograd/_functions/tensor.py\", line 30\n    def forward(ctx, tensor, sizes):\n        ctx.sizes = sizes\n        ctx.numel = reduce(lambda x, y: x * y, sizes, 1)\n                           ~ <--- HERE\n        if tensor.numel() != ctx.numel:\n            raise RuntimeError((\"requested resize to {} ({} elements in total), \"\n'Resize' is being compiled since it was called from 'Resize.forward'\n  File \"<ipython-input-11-d3a35c245136>\", line 8\n    def forward(self, x):\n        return AutoResize.apply(x, self.new_size)\n               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedNodeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d3a35c245136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscripted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscripted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/__init__.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_script_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0mqualified_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mconcrete_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcrete_type_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconcrete_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mcreate_methods_from_stubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_emit_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcrete_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_methods_from_stubs\u001b[0;34m(concrete_type, stubs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mrcbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution_callback\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_method\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mconcrete_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_script_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/__init__.py\u001b[0m in \u001b[0;36m_compile_and_register_class\u001b[0;34m(obj, rcb, qualified_name)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_compile_and_register_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqualified_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m     \u001b[0mast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_jit_class_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m     \u001b[0m_jit_script_class_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0m_add_script_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqualified_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36mget_jit_class_def\u001b[0;34m(cls, self_name)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     method_defs = [get_jit_def(method[1],\n\u001b[0;32m--> 137\u001b[0;31m                    self_name=self_name) for method in methods]\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0msourcelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_source_lines_and_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mErrorReport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     method_defs = [get_jit_def(method[1],\n\u001b[0;32m--> 137\u001b[0;31m                    self_name=self_name) for method in methods]\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0msourcelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_source_lines_and_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mErrorReport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36mget_jit_def\u001b[0;34m(fn, self_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mtype_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSourceContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleading_whitespace_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_uses_true_division\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_ast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36mbuild_def\u001b[0;34m(ctx, py_def, type_line, self_name)\u001b[0m\n\u001b[1;32m    189\u001b[0m     return Def(Ident(r, py_def.name),\n\u001b[1;32m    190\u001b[0m                \u001b[0mdecl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                build_stmts(ctx, body))\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36mbuild_stmts\u001b[0;34m(ctx, stmts)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_stmts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstmts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mstmts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbuild_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstmts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstmts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_stmts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstmts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mstmts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbuild_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstmts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstmts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ctx, node)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnsupportedNodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36mbuild_Assign\u001b[0;34m(ctx, stmt)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_Assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mrhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mlhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mAssign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ctx, node)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnsupportedNodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36mbuild_Call\u001b[0;34m(ctx, expr)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_Call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbuild_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_arg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpy_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'starargs'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mstararg_expr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_Call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbuild_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_arg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpy_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'starargs'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mstararg_expr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/pytorch-dev/pytorch-convtranspose/torch/jit/frontend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ctx, node)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'build_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mUnsupportedNodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnsupportedNodeError\u001b[0m: Lambda aren't supported:\n  File \"/home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/autograd/_functions/tensor.py\", line 30\n    def forward(ctx, tensor, sizes):\n        ctx.sizes = sizes\n        ctx.numel = reduce(lambda x, y: x * y, sizes, 1)\n                           ~ <--- HERE\n        if tensor.numel() != ctx.numel:\n            raise RuntimeError((\"requested resize to {} ({} elements in total), \"\n'Resize' is being compiled since it was called from 'Resize.forward'\n  File \"<ipython-input-11-d3a35c245136>\", line 8\n    def forward(self, x):\n        return AutoResize.apply(x, self.new_size)\n               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd._functions import Resize as AutoResize\n",
    "\n",
    "class Resize(nn.Module):\n",
    "    def __init__(self, new_size):\n",
    "        super(Resize, self).__init__()\n",
    "        self.new_size = new_size\n",
    "    def forward(self, x):\n",
    "        return AutoResize.apply(x, self.new_size)\n",
    "    \n",
    "resize = Resize((3, 8))\n",
    "scripted = torch.jit.script(resize)\n",
    "print(scripted._c.dump_to_str(True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12., -11., -10.,  -9.,  -8.,  -7.,  -6.,  -5.],\n",
       "        [ -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.],\n",
       "        [  4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.resize(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module __torch__.torch.nn.modules.activation.Tanh {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.torch.nn.modules.activation.Tanh,\n",
      "            %input.1 : Tensor):\n",
      "        %3 : Tensor = aten::tanh(%input.1) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:325:15\n",
      "        return (%3)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tanh = torch.nn.Tanh()\n",
    "scripted = torch.jit.script(tanh)\n",
    "print(scripted._c.dump_to_str(True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module __torch__.torch.nn.modules.container.Sequential {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    0 = <__torch__.torch.nn.modules.conv.Conv2d object at 0x55f06e5a9cc0>\n",
      "    1 = <__torch__.torch.nn.modules.activation.ReLU object at 0x55f06e5ce770>\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.torch.nn.modules.container.Sequential,\n",
      "            %input.1 : Tensor):\n",
      "        %3 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=\"0\"](%self)\n",
      "        %5 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name=\"1\"](%self)\n",
      "        %input.3 : Tensor = prim::CallMethod[name=\"forward\"](%3, %input.1) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/container.py:100:20\n",
      "        %input.5 : Tensor = prim::CallMethod[name=\"forward\"](%5, %input.3) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/container.py:100:20\n",
      "        return (%input.5)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "    module __torch__.torch.nn.modules.conv.Conv2d {\n",
      "      parameters {\n",
      "        weight = ...\n",
      "        bias = ...\n",
      "      }\n",
      "      attributes {\n",
      "        weight = ...\n",
      "        bias = ...\n",
      "        training = True\n",
      "        transposed = False\n",
      "        _padding_repeated_twice = (0, 0, 0, 0)\n",
      "      }\n",
      "      methods {\n",
      "        method forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.conv.Conv2d,\n",
      "                %input.1 : Tensor):\n",
      "            %3 : Tensor = prim::GetAttr[name=\"weight\"](%self)\n",
      "            %4 : Tensor = prim::CallMethod[name=\"_conv_forward\"](%self, %input.1, %3) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/conv.py:351:15\n",
      "            return (%4)\n",
      "      \n",
      "        }\n",
      "        method _conv_forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.conv.Conv2d,\n",
      "                %input.1 : Tensor,\n",
      "                %weight.1 : Tensor):\n",
      "            %19 : int = prim::Constant[value=1]() # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/conv.py:345:47\n",
      "            %22 : int = prim::Constant[value=0]() # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/conv.py:346:34\n",
      "            %33 : Tensor? = prim::GetAttr[name=\"bias\"](%self)\n",
      "            %44 : int[] = prim::ListConstruct(%19, %19)\n",
      "            %45 : int[] = prim::ListConstruct(%22, %22)\n",
      "            %46 : int[] = prim::ListConstruct(%19, %19)\n",
      "            %47 : Tensor = aten::conv2d(%input.1, %weight.1, %33, %44, %45, %46, %19) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/conv.py:347:15\n",
      "            return (%47)\n",
      "      \n",
      "        }\n",
      "      }\n",
      "      submodules {\n",
      "      }\n",
      "    }\n",
      "    module __torch__.torch.nn.modules.activation.ReLU {\n",
      "      parameters {\n",
      "      }\n",
      "      attributes {\n",
      "        training = True\n",
      "      }\n",
      "      methods {\n",
      "        method forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.activation.ReLU,\n",
      "                %input.1 : Tensor):\n",
      "            %4 : Function = prim::Constant[name=\"relu\"]()\n",
      "            %3 : bool = prim::Constant[value=0]() # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:97:37\n",
      "            %5 : Tensor = prim::CallFunction(%4, %input.1, %3) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:97:15\n",
      "            return (%5)\n",
      "      \n",
      "        }\n",
      "      }\n",
      "      submodules {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv_relu = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 3, 3),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "scripted = torch.jit.script(conv_relu)\n",
    "print(scripted._c.dump_to_str(True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module __torch__.Clamp {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    min = -4\n",
      "    max = 4\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.Clamp,\n",
      "            %x.1 : Tensor):\n",
      "        %3 : int = prim::GetAttr[name=\"min\"](%self)\n",
      "        %4 : int = prim::GetAttr[name=\"max\"](%self)\n",
      "        %5 : Tensor = aten::clamp(%x.1, %3, %4) # <ipython-input-15-af845023880d>:10:15\n",
      "        return (%5)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Clamp(torch.nn.Module):\n",
    "    def __init__(self, mn, mx):\n",
    "        super(Clamp, self).__init__()\n",
    "        self.min = mn\n",
    "        self.max = mx\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(x, self.min, self.max)\n",
    "clamp = Clamp(-4, 4)\n",
    "scripted = torch.jit.script(clamp)\n",
    "print(scripted._c.dump_to_str(True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module __torch__.LinearRelu {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    linear = <__torch__.torch.nn.modules.linear.___torch_mangle_1.Linear object at 0x55f06e5cba20>\n",
      "    relu = <__torch__.torch.nn.modules.activation.ReLU object at 0x55f06e5cdad0>\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.LinearRelu,\n",
      "            %x.1 : Tensor):\n",
      "        %2 : __torch__.torch.nn.modules.linear.___torch_mangle_1.Linear = prim::GetAttr[name=\"linear\"](%self)\n",
      "        %x.3 : Tensor = prim::CallMethod[name=\"forward\"](%2, %x.1) # <ipython-input-16-2af3bb9cfc73>:8:12\n",
      "        %5 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name=\"relu\"](%self)\n",
      "        %7 : Tensor = prim::CallMethod[name=\"forward\"](%5, %x.3) # <ipython-input-16-2af3bb9cfc73>:9:15\n",
      "        return (%7)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "    module __torch__.torch.nn.modules.linear.___torch_mangle_1.Linear {\n",
      "      parameters {\n",
      "        weight = ...\n",
      "        bias = ...\n",
      "      }\n",
      "      attributes {\n",
      "        weight = ...\n",
      "        bias = ...\n",
      "        training = True\n",
      "      }\n",
      "      methods {\n",
      "        method forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.linear.___torch_mangle_1.Linear,\n",
      "                %input.1 : Tensor):\n",
      "            %5 : Function = prim::Constant[name=\"linear\"]()\n",
      "            %3 : Tensor = prim::GetAttr[name=\"weight\"](%self)\n",
      "            %4 : Tensor = prim::GetAttr[name=\"bias\"](%self)\n",
      "            %6 : Tensor = prim::CallFunction(%5, %input.1, %3, %4) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/linear.py:87:15\n",
      "            return (%6)\n",
      "      \n",
      "        }\n",
      "      }\n",
      "      submodules {\n",
      "      }\n",
      "    }\n",
      "    module __torch__.torch.nn.modules.activation.ReLU {\n",
      "      parameters {\n",
      "      }\n",
      "      attributes {\n",
      "        training = True\n",
      "      }\n",
      "      methods {\n",
      "        method forward {\n",
      "          graph(%self : __torch__.torch.nn.modules.activation.ReLU,\n",
      "                %input.1 : Tensor):\n",
      "            %4 : Function = prim::Constant[name=\"relu\"]()\n",
      "            %3 : bool = prim::Constant[value=0]() # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:97:37\n",
      "            %5 : Tensor = prim::CallFunction(%4, %input.1, %3) # /home/zafar/Git/pytorch-dev/pytorch-convtranspose/torch/nn/modules/activation.py:97:15\n",
      "            return (%5)\n",
      "      \n",
      "        }\n",
      "      }\n",
      "      submodules {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class LinearRelu(nn.Module):\n",
    "    def __init__(self, iC, oC):\n",
    "        super(LinearRelu, self).__init__()\n",
    "        self.linear = nn.Linear(iC, oC)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.relu(x)\n",
    "    \n",
    "linear_relu = LinearRelu(3, 4)\n",
    "scripted = torch.jit.script(linear_relu)\n",
    "print(scripted._c.dump_to_str(True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4471, -0.4471, -0.4471, -0.4471, -0.4471, -0.4471, -0.4471, -0.4471,\n",
      "         -0.4471, -0.4471,  0.0000,  0.9686,  2.0118,  2.9804,  4.0235,  4.9922,\n",
      "          6.0353,  7.0039,  7.9725,  9.0157]], size=(1, 20),\n",
      "       dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.07450980693101883, zero_point=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[255., 255.,   0.,   4.]], size=(1, 4), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=1.0, zero_point=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH = 1\n",
    "N = 20\n",
    "OP = 4\n",
    "\n",
    "x = (torch.arange(BATCH*N).to(torch.float) - BATCH*N // 2)\n",
    "x = x.resize(BATCH, N)\n",
    "s, zp = qparams(x, torch.qint8)\n",
    "qx = torch.quantize_per_tensor(x, s, zp, torch.quint8)\n",
    "print(qx)\n",
    "linear = nn.quantized.Linear(N, OP)\n",
    "linear(qx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module __torch__.___torch_mangle_5.MyModel {\n",
      "  parameters {\n",
      "  }\n",
      "  attributes {\n",
      "    training = True\n",
      "    s = [3]\n",
      "    p = [5]\n",
      "    d = [7]\n",
      "    g = 11\n",
      "  }\n",
      "  methods {\n",
      "    method forward {\n",
      "      graph(%self : __torch__.___torch_mangle_5.MyModel,\n",
      "            %x.1 : Tensor):\n",
      "        %10 : bool = prim::Constant[value=0]() # <ipython-input-25-169a86a7cf93>:11:66\n",
      "        %3 : None = prim::Constant() # <ipython-input-25-169a86a7cf93>:10:53\n",
      "        %6 : int = prim::Constant[value=0]() # <ipython-input-25-169a86a7cf93>:10:76\n",
      "        %4 : int[] = prim::GetAttr[name=\"s\"](%self)\n",
      "        %5 : int[] = prim::GetAttr[name=\"p\"](%self)\n",
      "        %7 : int[] = prim::ListConstruct(%6, %6)\n",
      "        %8 : int[] = prim::GetAttr[name=\"d\"](%self)\n",
      "        %9 : int = prim::GetAttr[name=\"g\"](%self)\n",
      "        %11 : Tensor = quantized::conv2d_prepack(%x.1, %3, %4, %5, %7, %8, %9, %10) # <ipython-input-25-169a86a7cf93>:10:15\n",
      "        return (%11)\n",
      "  \n",
      "    }\n",
      "  }\n",
      "  submodules {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, s, p, d, g):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.s = s\n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.g = g\n",
    "    def forward(self, x):\n",
    "        return torch.ops.quantized.conv2d_prepack(x, None, self.s, self.p, [0, 0], \n",
    "                                                  self.d, self.g, False)\n",
    "    \n",
    "model = MyModel([3], [5], [7], 11)\n",
    "scripted = torch.jit.script(model)\n",
    "print(scripted._c.dump_to_str(True, False, False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
